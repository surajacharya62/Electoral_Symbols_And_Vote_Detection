{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Oslo\\\\OsloMet\\\\Fourth semester\\\\Electoral_Symbols_And_Vote_Detection_MLOPS\\\\Electoral_Symbols_And_Vote_Detection\\\\research\\\\faster-rcnn'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Oslo\\\\OsloMet\\\\Fourth semester\\\\Electoral_Symbols_And_Vote_Detection_MLOPS\\\\Electoral_Symbols_And_Vote_Detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #-------initializing the model----- \n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    root_dir: Path\n",
    "    path_of_model: Path\n",
    "    test_images_path: Path\n",
    "    annotations_path: Path\n",
    "    faster_rcnn_files_path: Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str\n",
    "    params_image_size: list\n",
    "    params_batch_size: int\n",
    "    classes: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules.symbol_detection.faster_rcnn.constants import *\n",
    "from src.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "                self,\n",
    "                config_filepath =  CONFIG_FILE_PATH,\n",
    "                params_filepath = PARAMS_FILE_PATH\n",
    "        ):\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            self.params = read_yaml(params_filepath)\n",
    "\n",
    "            create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_evaluation_config(self)->EvaluationConfig:\n",
    "      \n",
    "      config = self.config.trained_models\n",
    "\n",
    "      eval_config = EvaluationConfig(\n",
    "                root_dir = Path(config.root_dir),\n",
    "                path_of_model=Path(config.model_path),\n",
    "                test_images_path=Path(config.test_images_path),\n",
    "                annotations_path=Path(config.annotations_path),\n",
    "                faster_rcnn_files_path=Path(config.faster_rcnn_files_path),\n",
    "                mlflow_uri=\"\",\n",
    "                all_params=self.params,\n",
    "                params_image_size=self.params.IMAGE_SIZE,\n",
    "                params_batch_size=self.params.BATCH_SIZE,\n",
    "                classes=self.params.CLASSES                \n",
    "          )\n",
    "      \n",
    "      return eval_config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.symbol_detection.faster_rcnn.components.electoral_symbol_dataset import  ElectoralSymbolDataset\n",
    "from modules.symbol_detection.faster_rcnn.components.visualize_symbols_detection import VisualizePrediction\n",
    "from modules.symbol_detection.faster_rcnn.components.compare_bounding_boxes_faster import CompareBoundingBox\n",
    "from modules.symbol_detection.faster_rcnn.components.reshape_data import ReshapeData\n",
    "from modules.symbol_detection.faster_rcnn.components.metrics import Metrics\n",
    "from modules.symbol_detection.faster_rcnn.utils.faster_rcnn_utils import label_to_id,get_transform,collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "class Evalaution:\n",
    "    def __init__(self, config:EvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_data_loader(self):\n",
    "\n",
    "        annotation_labels = label_to_id(self.config.annotations_path)\n",
    "        test_set = ElectoralSymbolDataset(        \n",
    "            self.config.test_images_path,\n",
    "            \"single_image\",\n",
    "            self.config.annotations_path,\n",
    "            annotation_labels,\n",
    "            get_transform(train=False),\n",
    "            is_single_image=False \n",
    "        )\n",
    "\n",
    "        test_data_loader = DataLoader(test_set, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        return test_set, test_data_loader\n",
    "    \n",
    "    \n",
    "    def get_faster_rcnn_model(self):\n",
    "        \"\"\"\n",
    "        Initializing model  \n",
    "        \"\"\"\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True)    \n",
    "        # get number of input features for the classifier\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, self.config.classes) \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def make_predictions(self,dataset_loader, faster_rcnn_model):\n",
    "        \"\"\"\n",
    "        Make predictions on test images\n",
    "        \"\"\"\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')       \n",
    "        faster_rcnn_model.load_state_dict(torch.load(self.config.path_of_model, map_location=device))\n",
    "        faster_rcnn_model.eval()\n",
    "        # test_data_loader = self.get_data_loader()\n",
    "\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for images, imageids, imagenames, target in dataset_loader:\n",
    "                images = [image.to(device) for image in images]\n",
    "                outputs = faster_rcnn_model(images)\n",
    "\n",
    "                for output, imageid, imagename in zip(outputs, imageids, imagenames):\n",
    "                    prediction = (output, imageid, imagename)\n",
    "                    predictions.append(prediction)\n",
    "        \n",
    "        test_images_path = self.config.test_images_path\n",
    "        test_images_name = []\n",
    "        for filename in os.listdir(test_images_path):\n",
    "            test_images_name.append(filename) \n",
    "        \n",
    "        return predictions, test_images_name\n",
    "    \n",
    "    \n",
    "    def visualize_predictions(self, predictions, test_set):\n",
    "        \"\"\"\n",
    "            Visualize prediction\n",
    "        \"\"\" \n",
    "            \n",
    "        visualize = VisualizePrediction()\n",
    "        annotation_labels = label_to_id(self.config.annotations_path)\n",
    "\n",
    "        visualize.visualize_predicted_images(self.config.test_images_path, test_set, predictions, annotation_labels)\n",
    "    \n",
    "    \n",
    "    def vote_validation(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def metrics_calculation(self, test_set, predictions):\n",
    "    \n",
    "        annotation_labels = label_to_id(self.config.annotations_path)\n",
    "        #Predictions Bounding Box Comparison        \n",
    "        compare_bboxes = CompareBoundingBox()\n",
    "        compare_bboxes.labels(test_set, predictions, annotation_labels)\n",
    "\n",
    "        #Data Reshaping\n",
    "        reshape_data = ReshapeData()\n",
    "        reshape_data.process_and_reshape_data_v2(self.config.faster_rcnn_files_path)\n",
    "\n",
    "        metrics = Metrics()\n",
    "        metrics.metrics(predictions, self.config.annotations_path, annotation_labels, self.config.faster_rcnn_files_path)\n",
    "        metrics.call_metrics(self.config.faster_rcnn_files_path)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-17 21:01:41,105: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-11-17 21:01:41,107: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-11-17 21:01:41,109: INFO: common: created directory at: artifacts]\n",
      "Length of dataset: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suraj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\suraj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         ...,\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "        [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         ...,\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "        [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         ...,\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]), tensor([0]), 'image_0000.jpg', {'boxes': tensor([[ 230., 1580.,  419., 1729.],\n",
      "        [ 608., 1580.,  797., 1729.],\n",
      "        [ 986., 1580., 1175., 1729.],\n",
      "        [1364., 1580., 1553., 1729.],\n",
      "        [1742., 1580., 1931., 1729.],\n",
      "        [2120., 1580., 2309., 1729.],\n",
      "        [ 230., 1769.,  419., 1918.],\n",
      "        [ 608., 1769.,  797., 1918.],\n",
      "        [ 986., 1769., 1175., 1918.],\n",
      "        [1364., 1769., 1553., 1918.],\n",
      "        [1742., 1769., 1931., 1918.],\n",
      "        [2120., 1769., 2309., 1918.],\n",
      "        [ 230., 1958.,  419., 2107.],\n",
      "        [ 608., 1958.,  797., 2107.],\n",
      "        [ 986., 1958., 1175., 2107.],\n",
      "        [1364., 1958., 1553., 2107.],\n",
      "        [1742., 1958., 1931., 2107.],\n",
      "        [2120., 1958., 2309., 2107.],\n",
      "        [ 230., 2147.,  419., 2296.],\n",
      "        [ 608., 2147.,  797., 2296.],\n",
      "        [ 986., 2147., 1175., 2296.],\n",
      "        [1364., 2147., 1553., 2296.],\n",
      "        [1742., 2147., 1931., 2296.],\n",
      "        [2120., 2147., 2309., 2296.],\n",
      "        [ 230., 2336.,  419., 2485.],\n",
      "        [ 608., 2336.,  797., 2485.],\n",
      "        [ 986., 2336., 1175., 2485.],\n",
      "        [1364., 2336., 1553., 2485.],\n",
      "        [1742., 2336., 1931., 2485.],\n",
      "        [2120., 2336., 2309., 2485.],\n",
      "        [ 230., 2525.,  419., 2674.],\n",
      "        [ 608., 2525.,  797., 2674.],\n",
      "        [ 986., 2525., 1175., 2674.],\n",
      "        [1364., 2525., 1553., 2674.],\n",
      "        [1742., 2525., 1931., 2674.],\n",
      "        [2120., 2525., 2309., 2674.],\n",
      "        [ 230., 2714.,  419., 2863.],\n",
      "        [ 608., 2714.,  797., 2863.],\n",
      "        [ 986., 2714., 1175., 2863.],\n",
      "        [1364., 2714., 1553., 2863.],\n",
      "        [1742., 2714., 1931., 2863.],\n",
      "        [2120., 2714., 2309., 2863.],\n",
      "        [1145., 2156., 1295., 2306.]]), 'labels': tensor([43, 23, 26, 27, 10, 21, 19, 12, 17, 33, 34,  8, 29, 39, 42, 28, 25,  3,\n",
      "        20, 41, 18,  9, 37, 22, 14, 24, 13,  4, 16,  6,  7,  2, 32,  1, 38, 31,\n",
      "        44, 35, 11,  5, 30, 36, 40]), 'image_id': tensor([0]), 'area': tensor([28161., 28161., 28161., 28161., 28161., 28161., 28161., 28161., 28161.,\n",
      "        28161., 28161., 28161., 28161., 28161., 28161., 28161., 28161., 28161.,\n",
      "        28161., 28161., 28161., 28161., 28161., 28161., 28161., 28161., 28161.,\n",
      "        28161., 28161., 28161., 28161., 28161., 28161., 28161., 28161., 28161.,\n",
      "        28161., 28161., 28161., 28161., 28161., 28161., 22500.]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}) ({'boxes': tensor([[2122.1592, 2335.2842, 2308.1880, 2486.8118],\n",
      "        [ 990.2068, 1768.4418, 1169.9338, 1912.5004],\n",
      "        [ 984.5367, 2526.2102, 1174.9352, 2674.4292],\n",
      "        [1366.4507, 2524.0610, 1556.5103, 2672.6865],\n",
      "        [1363.6595, 2150.5183, 1561.5316, 2288.9546],\n",
      "        [2115.1040, 2523.1104, 2304.6328, 2672.7788],\n",
      "        [1744.1655, 2712.5005, 1933.3409, 2862.5898],\n",
      "        [ 606.4094, 1770.8696,  799.0162, 1918.1129],\n",
      "        [ 606.0380, 2148.2168,  798.9850, 2297.3313],\n",
      "        [1361.5450, 2712.5400, 1553.0881, 2863.7988],\n",
      "        [2118.5974, 2154.5896, 2303.4241, 2297.8049],\n",
      "        [ 230.6584, 1768.2131,  419.3571, 1919.0903],\n",
      "        [1739.3778, 1769.0859, 1930.4722, 1919.3995],\n",
      "        [ 232.5105, 1580.6520,  419.2076, 1731.8884],\n",
      "        [ 605.5041, 2337.9011,  798.3550, 2484.6768],\n",
      "        [ 989.3367, 2143.9922, 1178.5389, 2303.6248],\n",
      "        [1743.3077, 1577.7408, 1930.1716, 1730.3831],\n",
      "        [ 610.1277, 2526.3906,  798.2098, 2674.4136],\n",
      "        [ 610.3668, 2716.7295,  795.1953, 2861.5964],\n",
      "        [1366.3368, 2336.9326, 1557.3091, 2486.0525],\n",
      "        [ 231.8112, 2710.6633,  413.9268, 2859.7605],\n",
      "        [ 609.7189, 1957.9608,  799.6226, 2106.9863],\n",
      "        [ 990.3007, 2338.2722, 1183.7335, 2487.8938],\n",
      "        [1741.0748, 2524.3103, 1931.6083, 2668.8916],\n",
      "        [ 984.6697, 1949.4949, 1174.7534, 2103.0503],\n",
      "        [ 231.4009, 1957.8920,  418.0320, 2109.0698],\n",
      "        [2119.3625, 1772.2139, 2320.5417, 1917.2107],\n",
      "        [ 234.9993, 2148.2847,  426.3294, 2298.7031],\n",
      "        [1145.8569, 2155.6782, 1294.2266, 2304.9619],\n",
      "        [1736.4161, 2143.9001, 1934.2831, 2298.1560],\n",
      "        [2114.8435, 1962.3849, 2307.5842, 2114.0103],\n",
      "        [1739.9841, 1955.3328, 1936.4719, 2111.7410],\n",
      "        [ 983.7747, 2709.9211, 1181.2108, 2862.8403],\n",
      "        [1742.3391, 2334.1609, 1932.4564, 2482.8040],\n",
      "        [ 236.8708, 2335.1868,  419.5699, 2484.3176],\n",
      "        [1362.6094, 1579.0115, 1552.8461, 1727.7909],\n",
      "        [1364.2811, 1948.9362, 1552.0676, 2103.0715],\n",
      "        [ 608.2407, 1574.8037,  786.9529, 1728.0875],\n",
      "        [2120.2954, 2709.4185, 2306.8071, 2863.1868],\n",
      "        [1358.9391, 1765.3699, 1551.2487, 1923.3546],\n",
      "        [1365.8397, 1769.0101, 1554.3171, 1923.9009],\n",
      "        [2121.5618, 1580.5826, 2303.9434, 1726.7092],\n",
      "        [2121.0066, 1576.0660, 2314.6172, 1729.9817],\n",
      "        [1739.1071, 2337.1392, 1932.7589, 2480.5742],\n",
      "        [ 986.3589, 1584.4821, 1166.6185, 1731.0134],\n",
      "        [ 984.3658, 2714.2498, 1180.8549, 2854.9529],\n",
      "        [ 980.5456, 1573.3778, 1174.8435, 1730.0092],\n",
      "        [2123.9404, 2714.0850, 2315.4397, 2869.5312],\n",
      "        [2123.4485, 1581.8029, 2302.4951, 1730.9615],\n",
      "        [ 611.8984, 1579.0311,  804.9938, 1722.5686],\n",
      "        [2120.2244, 1963.0405, 2299.4653, 2107.1711],\n",
      "        [ 236.5729, 2340.8369,  409.2201, 2474.6816],\n",
      "        [1747.3507, 2339.8108, 1925.5997, 2483.3352],\n",
      "        [1148.3979, 2155.9597, 1295.1328, 2305.0430],\n",
      "        [1743.3900, 2143.8862, 1926.9612, 2298.2024],\n",
      "        [ 985.9697, 1575.7871, 1165.6924, 1729.7644],\n",
      "        [2126.3362, 2715.1707, 2308.5735, 2865.0774],\n",
      "        [ 234.7146, 2528.2095,  415.1626, 2676.8379],\n",
      "        [ 214.6105, 2528.3916,  423.6292, 2676.1550],\n",
      "        [1371.3389, 1765.3367, 1553.3623, 1920.2579],\n",
      "        [2108.0364, 2727.8799, 2296.7451, 2865.0112],\n",
      "        [1368.3699, 1585.2760, 1551.1755, 1728.4712],\n",
      "        [1749.2627, 1957.8297, 1925.5316, 2106.6702],\n",
      "        [ 977.9842, 1570.7693, 1174.5933, 1737.5490],\n",
      "        [ 240.3975, 2341.2949,  413.6667, 2487.0955],\n",
      "        [1357.5764, 1581.4628, 1552.1116, 1724.9254],\n",
      "        [1747.6248, 2344.5801, 1924.8470, 2482.2488],\n",
      "        [1744.3467, 1958.0999, 1922.9182, 2103.0637],\n",
      "        [ 235.6192, 2338.0332,  419.5698, 2485.3833],\n",
      "        [ 230.6234, 2526.8755,  416.6139, 2667.9651],\n",
      "        [1363.9681, 1960.8529, 1552.9858, 2107.6050],\n",
      "        [ 229.8307, 2527.7043,  411.2323, 2666.7258],\n",
      "        [2114.2585, 1769.9055, 2322.7034, 1921.8037],\n",
      "        [2126.5603, 1965.0840, 2291.0881, 2104.4805],\n",
      "        [ 222.6427, 2358.0044,  395.8764, 2488.0874],\n",
      "        [ 229.2303, 1955.1971,  419.8658, 2101.1665],\n",
      "        [ 605.7454, 1571.2139,  799.3492, 1729.5482],\n",
      "        [ 235.6422, 2151.0334,  436.1622, 2299.7705],\n",
      "        [ 613.2123, 1572.6755,  800.6747, 1729.1316],\n",
      "        [2111.5442, 1769.2001, 2323.1506, 1913.6178],\n",
      "        [ 227.8597, 2538.6877,  415.1304, 2672.6396],\n",
      "        [1363.2959, 1955.6635, 1550.5869, 2108.3506],\n",
      "        [ 238.4898, 2520.2026,  418.9515, 2674.5979],\n",
      "        [1368.8601, 1956.2607, 1541.9512, 2106.1548],\n",
      "        [1358.7539, 1961.5770, 1562.5706, 2111.3608],\n",
      "        [ 990.8278, 1590.2328, 1164.2334, 1733.4031],\n",
      "        [1374.5068, 1956.0258, 1559.6168, 2103.3379],\n",
      "        [ 223.4132, 2528.4370,  421.2932, 2668.4014],\n",
      "        [1366.2073, 1583.4706, 1554.6516, 1737.1862],\n",
      "        [ 235.4025, 2146.4880,  416.4965, 2294.5010],\n",
      "        [1731.0970, 2337.5046, 1939.2772, 2485.4138],\n",
      "        [1378.4137, 2324.4517, 1551.4767, 2491.5845],\n",
      "        [ 977.1375, 1947.6323, 1169.2725, 2107.0315],\n",
      "        [1740.1198, 2530.3567, 1937.8906, 2666.8535],\n",
      "        [ 233.7352, 1962.8516,  420.9442, 2104.7322],\n",
      "        [ 223.0474, 2153.6187,  414.9508, 2311.5725],\n",
      "        [ 215.4269, 2332.8425,  411.6864, 2482.0190],\n",
      "        [1364.6489, 1958.2728, 1548.2858, 2106.4263]]), 'labels': tensor([ 6, 17, 32,  1,  9, 31, 30, 12, 41,  5, 22, 19, 34, 43, 24, 18, 10,  2,\n",
      "        35,  4, 44, 39, 13, 38, 42, 29,  8, 20, 40, 37,  3, 25, 11, 16, 18, 27,\n",
      "         8, 23, 20, 29, 33, 21, 35, 20, 43, 19, 26, 36, 38,  7,  9,  4, 21, 15,\n",
      "        38, 27, 21,  8, 18, 13, 26, 43,  8,  7,  2, 44,  9, 28, 26,  7, 38, 42,\n",
      "        21, 21, 24, 33, 38, 13, 26, 38, 10, 31,  3, 28, 21, 24, 25,  5,  5, 18,\n",
      "        38, 17, 25, 21, 17, 14, 14, 35]), 'scores': tensor([0.9952, 0.9915, 0.9902, 0.9874, 0.9868, 0.9855, 0.9850, 0.9836, 0.9823,\n",
      "        0.9789, 0.9724, 0.9714, 0.9695, 0.9647, 0.9646, 0.9629, 0.9564, 0.9479,\n",
      "        0.9420, 0.9418, 0.9396, 0.9364, 0.8979, 0.8874, 0.8777, 0.8516, 0.8224,\n",
      "        0.8044, 0.7845, 0.7779, 0.7405, 0.7239, 0.6966, 0.6776, 0.6315, 0.6257,\n",
      "        0.5972, 0.5169, 0.4494, 0.4472, 0.4270, 0.4124, 0.3925, 0.3258, 0.3177,\n",
      "        0.3153, 0.3091, 0.2760, 0.2665, 0.2546, 0.2444, 0.2424, 0.2375, 0.2349,\n",
      "        0.2343, 0.2309, 0.2261, 0.2064, 0.1934, 0.1901, 0.1830, 0.1806, 0.1700,\n",
      "        0.1623, 0.1559, 0.1537, 0.1477, 0.1388, 0.1373, 0.1353, 0.1334, 0.1315,\n",
      "        0.1244, 0.1114, 0.1109, 0.1034, 0.0963, 0.0953, 0.0938, 0.0903, 0.0880,\n",
      "        0.0820, 0.0795, 0.0736, 0.0736, 0.0735, 0.0733, 0.0690, 0.0689, 0.0681,\n",
      "        0.0670, 0.0656, 0.0645, 0.0600, 0.0584, 0.0580, 0.0554, 0.0504])}, tensor([0]), 'image_0000.jpg')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str, bytes, or os.PathLike object, not 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     evaluate\u001b[38;5;241m.\u001b[39mmetrics_calculation(test_set,predictions)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e \n",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mget_faster_rcnn_model()\n\u001b[0;32m      8\u001b[0m     predictions,images_name\u001b[38;5;241m=\u001b[39mevaluate\u001b[38;5;241m.\u001b[39mmake_predictions(dataset_loader, model)\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     evaluate\u001b[38;5;241m.\u001b[39mmetrics_calculation(test_set,predictions)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[10], line 79\u001b[0m, in \u001b[0;36mEvalaution.visualize_predictions\u001b[1;34m(self, predictions, images_name)\u001b[0m\n\u001b[0;32m     76\u001b[0m visualize \u001b[38;5;241m=\u001b[39m VisualizePrediction()\n\u001b[0;32m     77\u001b[0m annotation_labels \u001b[38;5;241m=\u001b[39m label_to_id(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mannotations_path)\n\u001b[1;32m---> 79\u001b[0m \u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_predicted_images\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_images_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimages_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Oslo\\OsloMet\\Fourth semester\\Electoral_Symbols_And_Vote_Detection_MLOPS\\Electoral_Symbols_And_Vote_Detection\\src\\modules\\symbol_detection\\faster_rcnn\\components\\visualize_symbols_detection.py:54\u001b[0m, in \u001b[0;36mVisualizePrediction.visualize_predicted_images\u001b[1;34m(self, test_images_path, test_images_name, predicted_labels, label_to_id)\u001b[0m\n\u001b[0;32m     52\u001b[0m image \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_set,label)\n\u001b[1;32m---> 54\u001b[0m image \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_images_path, image)            \n\u001b[0;32m     55\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image)                       \n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# img_np = image.permute(1, 2, 0).numpy()         \u001b[39;00m\n",
      "File \u001b[1;32m<frozen ntpath>:147\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n",
      "File \u001b[1;32m<frozen genericpath>:152\u001b[0m, in \u001b[0;36m_check_arg_types\u001b[1;34m(funcname, *args)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: join() argument must be str, bytes, or os.PathLike object, not 'Tensor'"
     ]
    }
   ],
   "source": [
    "#pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    evaluation_config = config.get_evaluation_config() \n",
    "    evaluate = Evalaution(config=evaluation_config)\n",
    "    test_set, dataset_loader = evaluate.get_data_loader()\n",
    "    model = evaluate.get_faster_rcnn_model()\n",
    "    predictions,images_name=evaluate.make_predictions(dataset_loader, model)\n",
    "    evaluate.visualize_predictions(predictions, test_set)\n",
    "    evaluate.metrics_calculation(test_set,predictions)\n",
    "except Exception as e:\n",
    "    raise e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
